/*
 * FLUX Metal Acceleration
 *
 * GPU-accelerated matrix operations using Apple Metal Performance Shaders.
 * Provides significant speedup on Apple Silicon Macs.
 */

#ifndef FLUX_METAL_H
#define FLUX_METAL_H

#include <stddef.h>
#include <stdint.h>

#ifdef __cplusplus
extern "C" {
#endif

/*
 * Initialize Metal acceleration.
 * Returns 1 on success, 0 if Metal is not available.
 * Safe to call multiple times.
 */
int flux_metal_init(void);

/*
 * Check if Metal acceleration is available and initialized.
 */
int flux_metal_available(void);

/*
 * Cleanup Metal resources.
 */
void flux_metal_cleanup(void);

/*
 * GPU-accelerated matrix multiplication using MPS.
 * C[M,N] = alpha * A[M,K] @ B[K,N] + beta * C[M,N]
 *
 * transpose_a: if non-zero, use A^T
 * transpose_b: if non-zero, use B^T
 */
void flux_metal_sgemm(int transpose_a, int transpose_b,
                      int M, int N, int K,
                      float alpha,
                      const float *A, int lda,
                      const float *B, int ldb,
                      float beta,
                      float *C, int ldc);

/*
 * GPU-accelerated matrix multiplication with bf16 weights.
 * C[M,N] = alpha * A[M,K] @ B[K,N] + beta * C[M,N]
 *
 * A is f32, B is bf16 (weights), C is f32
 * This provides 2x memory bandwidth improvement for weight-bound operations.
 */
void flux_metal_sgemm_bf16(int transpose_a, int transpose_b,
                           int M, int N, int K,
                           float alpha,
                           const float *A, int lda,
                           const uint16_t *B_bf16, int ldb,
                           float beta,
                           float *C, int ldc);

/*
 * Batch matrix multiplication on GPU.
 * Performs batch_count independent matrix multiplications.
 */
void flux_metal_sgemm_batch(int transpose_a, int transpose_b,
                            int M, int N, int K,
                            float alpha,
                            const float *A, int lda, int stride_a,
                            const float *B, int ldb, int stride_b,
                            float beta,
                            float *C, int ldc, int stride_c,
                            int batch_count);

/*
 * Synchronize GPU operations (wait for completion).
 */
void flux_metal_sync(void);

/*
 * Begin a batch of GPU operations.
 * Operations after this call are encoded but not executed until flux_metal_end_batch().
 * This eliminates per-operation sync overhead.
 */
void flux_metal_begin_batch(void);

/*
 * End a batch of GPU operations.
 * Commits all encoded operations and waits for completion.
 */
void flux_metal_end_batch(void);

/*
 * Check if currently in batch mode.
 */
int flux_metal_in_batch(void);

/*
 * Get GPU memory usage info (for debugging).
 */
size_t flux_metal_memory_used(void);

/* ========================================================================
 * GPU Tensor API - Keep activations on GPU between operations
 * ======================================================================== */

/*
 * Opaque handle to a GPU-resident tensor.
 * Tensors are backed by pooled Metal buffers with shared storage mode,
 * allowing zero-copy access from both CPU and GPU on Apple Silicon.
 */
typedef struct flux_gpu_tensor *flux_gpu_tensor_t;

/*
 * Create a GPU tensor from CPU data.
 * Data is copied to GPU (or just referenced in shared memory mode).
 * Returns NULL on failure.
 */
flux_gpu_tensor_t flux_gpu_tensor_create(const float *data, size_t num_elements);

/*
 * Create an uninitialized GPU tensor (for output buffers).
 */
flux_gpu_tensor_t flux_gpu_tensor_alloc(size_t num_elements);

/*
 * Copy tensor data back to CPU.
 * Waits for any pending GPU operations on this tensor.
 */
void flux_gpu_tensor_read(flux_gpu_tensor_t tensor, float *out);

/*
 * Get direct pointer to tensor data (shared memory mode).
 * WARNING: Caller must ensure no GPU operations are pending on this tensor.
 * On Apple Silicon unified memory, this provides zero-copy access.
 */
float *flux_gpu_tensor_data(flux_gpu_tensor_t tensor);

/*
 * Release a GPU tensor back to the pool.
 */
void flux_gpu_tensor_free(flux_gpu_tensor_t tensor);

/*
 * Get tensor element count.
 */
size_t flux_gpu_tensor_size(flux_gpu_tensor_t tensor);

/* ========================================================================
 * GPU Operations on Tensors - Operations that keep data on GPU
 * ======================================================================== */

/*
 * Linear layer on GPU: out = x @ W^T + b (if b != NULL)
 * x: [seq_len, in_dim]
 * W: [out_dim, in_dim]
 * b: [out_dim] (can be NULL)
 * out: [seq_len, out_dim]
 *
 * Returns a new GPU tensor with the result.
 * Does NOT sync - GPU operation is queued.
 */
flux_gpu_tensor_t flux_gpu_linear(flux_gpu_tensor_t x,
                                   const float *W, const float *b,
                                   int seq_len, int in_dim, int out_dim);

/*
 * Sync all pending GPU operations.
 * Call this before reading tensor data or at step boundaries.
 */
void flux_gpu_sync(void);

/*
 * Begin a batch of GPU operations.
 * Operations are encoded but not executed until flux_gpu_batch_end().
 */
void flux_gpu_batch_begin(void);

/*
 * End batch and execute all queued operations.
 */
void flux_gpu_batch_end(void);

/*
 * GPU-accelerated scaled dot-product attention.
 * Computes attention for all heads in a single GPU batch.
 *
 * Q, K, V are in [heads, seq_q/seq_k, head_dim] layout (already transposed)
 * scores_scratch must be pre-allocated: [heads * seq_q * seq_k] floats
 * out will be [heads, seq_q, head_dim]
 *
 * This does: out = softmax(Q @ K^T * scale) @ V
 * Softmax is done on CPU (between two GPU batches).
 */
void flux_metal_attention(float *out,
                          const float *Q, const float *K, const float *V,
                          float *scores_scratch,
                          int heads, int seq_q, int seq_k, int head_dim,
                          float scale);

#ifdef __cplusplus
}
#endif

#endif /* FLUX_METAL_H */
