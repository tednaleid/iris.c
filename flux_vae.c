/*
 * FLUX VAE Implementation
 *
 * AutoencoderKLFlux2 - Variational Autoencoder for FLUX.2
 * Encodes images to latent space and decodes latents to images.
 *
 * Architecture:
 * - 32 latent channels (128 after patchification)
 * - 16x spatial compression
 * - Channel multipliers: [1, 2, 4, 4] -> [128, 256, 512, 512]
 * - GroupNorm (32 groups) + Swish activation
 */

#include "flux.h"
#include "flux_kernels.h"
#include "flux_safetensors.h"
#ifdef USE_METAL
#include "flux_metal.h"
#endif
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

/* ========================================================================
 * VAE Data Structures
 * ======================================================================== */

/* Residual block weights */
typedef struct {
    float *norm1_weight, *norm1_bias;   /* [channels] */
    float *conv1_weight, *conv1_bias;   /* [out_ch, in_ch, 3, 3] */
    float *norm2_weight, *norm2_bias;   /* [channels] */
    float *conv2_weight, *conv2_bias;   /* [out_ch, out_ch, 3, 3] */
    float *skip_weight, *skip_bias;     /* [out_ch, in_ch, 1, 1] if in_ch != out_ch */
    int in_channels;
    int out_channels;
} vae_resblock_t;

/* Self-attention block weights */
typedef struct {
    float *norm_weight, *norm_bias;     /* [channels] */
    float *q_weight, *q_bias;           /* [channels, channels, 1, 1] */
    float *k_weight, *k_bias;           /* [channels, channels, 1, 1] */
    float *v_weight, *v_bias;           /* [channels, channels, 1, 1] */
    float *out_weight, *out_bias;       /* [channels, channels, 1, 1] */
    int channels;
} vae_attnblock_t;

/* Downsample block (stride-2 conv) */
typedef struct {
    float *conv_weight, *conv_bias;     /* [channels, channels, 3, 3] */
    int channels;
} vae_downsample_t;

/* Upsample block (nearest + conv) */
typedef struct {
    float *conv_weight, *conv_bias;     /* [channels, channels, 3, 3] */
    int channels;
} vae_upsample_t;

/* VAE context */
typedef struct flux_vae {
    /* Configuration */
    int z_channels;         /* 32 */
    int base_channels;      /* 128 */
    int ch_mult[4];         /* {1, 2, 4, 4} */
    int num_res_blocks;     /* 2 */
    int num_groups;         /* 32 */
    float eps;              /* 1e-6 */

    /* Encoder weights */
    float *enc_conv_in_weight, *enc_conv_in_bias;   /* [128, 3, 3, 3] */

    /* Encoder down blocks: 4 levels, each with num_res_blocks + optional downsample */
    vae_resblock_t *enc_down_blocks;    /* 4 * 2 = 8 resblocks */
    vae_downsample_t *enc_downsample;   /* 3 downsamples (not at last level) */

    /* Encoder mid block */
    vae_resblock_t enc_mid_block1;
    vae_attnblock_t enc_mid_attn;
    vae_resblock_t enc_mid_block2;

    /* Encoder output */
    float *enc_norm_out_weight, *enc_norm_out_bias; /* [512] */
    float *enc_conv_out_weight, *enc_conv_out_bias; /* [64, 512, 3, 3] */

    /* Decoder weights */
    float *dec_conv_in_weight, *dec_conv_in_bias;   /* [512, 32, 3, 3] */

    /* Decoder mid block */
    vae_resblock_t dec_mid_block1;
    vae_attnblock_t dec_mid_attn;
    vae_resblock_t dec_mid_block2;

    /* Decoder up blocks: 4 levels, each with num_res_blocks+1 + optional upsample */
    vae_resblock_t *dec_up_blocks;      /* 4 * 3 = 12 resblocks */
    vae_upsample_t *dec_upsample;       /* 3 upsamples */

    /* Decoder output */
    float *dec_norm_out_weight, *dec_norm_out_bias; /* [128] */
    float *dec_conv_out_weight, *dec_conv_out_bias; /* [3, 128, 3, 3] */

    /* Normalization stats for latent space */
    float *bn_mean;         /* [128] */
    float *bn_var;          /* [128] */

    /* Post-quantization conv (1x1) applied before decoder */
    float *quant_conv_weight;       /* [64, 64, 1, 1] - encoder */
    float *quant_conv_bias;         /* [64] */
    float *post_quant_conv_weight;  /* [32, 32, 1, 1] - decoder */
    float *post_quant_conv_bias;    /* [32] */

    /* Working memory (allocated for max image size) */
    int max_h, max_w;
    float *work1, *work2, *work3;
    size_t work_size;
} flux_vae_t;

/* Forward declarations */
void flux_vae_free(flux_vae_t *vae);

/* ========================================================================
 * Helper Functions
 * ======================================================================== */

static void vae_conv2d(float *out, const float *in,
                       const float *weight, const float *bias,
                       int batch, int in_ch, int out_ch, int H, int W,
                       int kH, int kW, int stride, int padding) {
#ifdef USE_METAL
    if (!flux_metal_available()) {
        flux_metal_init();
    }
    if (flux_metal_available() &&
        flux_metal_conv2d(out, in, weight, bias,
                          batch, in_ch, out_ch, H, W,
                          kH, kW, stride, padding)) {
        return;
    }
#endif
    flux_conv2d(out, in, weight, bias, batch, in_ch, out_ch,
                H, W, kH, kW, stride, padding);
}

/* FLUX.2 VAE uses asymmetric padding for stride-2 downsampling convolutions:
 * pad right and bottom by 1, then do a VALID 3x3/stride-2 conv.
 *
 * This matches the reference implementation (e.g. diffusers' Downsample2D)
 * and avoids a ~7px top/left shift that shows up as a border in img2img. */
static void vae_pad_right_bottom(float *out, const float *in,
                                 int batch, int channels, int H, int W) {
    int Hp = H + 1;
    int Wp = W + 1;
    size_t in_plane = (size_t)H * (size_t)W;
    size_t out_plane = (size_t)Hp * (size_t)Wp;

    memset(out, 0, (size_t)batch * (size_t)channels * out_plane * sizeof(float));

    for (int b = 0; b < batch; b++) {
        for (int c = 0; c < channels; c++) {
            const float *src = in + ((size_t)b * (size_t)channels + (size_t)c) * in_plane;
            float *dst = out + ((size_t)b * (size_t)channels + (size_t)c) * out_plane;
            for (int y = 0; y < H; y++) {
                memcpy(dst + (size_t)y * (size_t)Wp,
                       src + (size_t)y * (size_t)W,
                       (size_t)W * sizeof(float));
            }
        }
    }
}

/* Swish activation in-place */
static void swish_inplace(float *x, int n) {
    flux_silu(x, n);
}

/* Apply residual block */
static void resblock_forward(float *out, const float *x,
                             const vae_resblock_t *block,
                             float *work, int batch, int H, int W,
                             int num_groups, float eps) {
    int in_ch = block->in_channels;
    int out_ch = block->out_channels;
    int spatial = H * W;

    /* Shortcut/skip connection */
    if (in_ch != out_ch) {
        /* 1x1 conv for channel adjustment */
        vae_conv2d(out, x, block->skip_weight, block->skip_bias,
                    batch, in_ch, out_ch, H, W, 1, 1, 1, 0);
    } else {
        flux_copy(out, x, batch * in_ch * spatial);
    }

    /* Main path: norm1 -> swish -> conv1 -> norm2 -> swish -> conv2 */

    /* GroupNorm + Swish */
    flux_group_norm(work, x, block->norm1_weight, block->norm1_bias,
                    batch, in_ch, H, W, num_groups, eps);
    swish_inplace(work, batch * in_ch * spatial);

    /* Conv1: in_ch -> out_ch */
    float *conv1_out = work + batch * in_ch * spatial;
    vae_conv2d(conv1_out, work, block->conv1_weight, block->conv1_bias,
                batch, in_ch, out_ch, H, W, 3, 3, 1, 1);

    /* GroupNorm + Swish */
    flux_group_norm(work, conv1_out, block->norm2_weight, block->norm2_bias,
                    batch, out_ch, H, W, num_groups, eps);
    swish_inplace(work, batch * out_ch * spatial);

    /* Conv2: out_ch -> out_ch */
    vae_conv2d(conv1_out, work, block->conv2_weight, block->conv2_bias,
                batch, out_ch, out_ch, H, W, 3, 3, 1, 1);

    /* Add residual */
    flux_add_inplace(out, conv1_out, batch * out_ch * spatial);
}

/* Apply self-attention block */
/* Returns 0 on success, -1 on OOM */
static int attnblock_forward(float *out, const float *x,
                             const vae_attnblock_t *block,
                             float *work, int batch, int H, int W,
                             int num_groups, float eps) {
    int ch = block->channels;
    int spatial = H * W;

    /* GroupNorm */
    flux_group_norm(work, x, block->norm_weight, block->norm_bias,
                    batch, ch, H, W, num_groups, eps);

    /* Project to Q, K, V using 1x1 convs */
    float *q = work + batch * ch * spatial;
    float *k = q + batch * ch * spatial;
    float *v = k + batch * ch * spatial;

    vae_conv2d(q, work, block->q_weight, block->q_bias,
                batch, ch, ch, H, W, 1, 1, 1, 0);
    vae_conv2d(k, work, block->k_weight, block->k_bias,
                batch, ch, ch, H, W, 1, 1, 1, 0);
    vae_conv2d(v, work, block->v_weight, block->v_bias,
                batch, ch, ch, H, W, 1, 1, 1, 0);

    /* Reshape: [B, C, H, W] -> [B, 1, HW, C] for attention */
    /* (We compute attention with heads=1 for simplicity) */
    float scale = 1.0f / sqrtf((float)ch);

    float *attn_out = v + batch * ch * spatial;

    /* Allocate attention work buffers once outside the batch loop */
    float *q_t = (float *)malloc(spatial * ch * sizeof(float));
    float *k_t = (float *)malloc(spatial * ch * sizeof(float));
    float *v_t = (float *)malloc(spatial * ch * sizeof(float));
    float *o_t = (float *)malloc(spatial * ch * sizeof(float));
    float *scores = (float *)malloc((size_t)spatial * spatial * sizeof(float));

    /* Check for allocation failures */
    if (!q_t || !k_t || !v_t || !o_t || !scores) {
        free(q_t);
        free(k_t);
        free(v_t);
        free(o_t);
        free(scores);
        return -1;  /* OOM */
    }

    for (int b = 0; b < batch; b++) {
        float *qb = q + b * ch * spatial;
        float *kb = k + b * ch * spatial;
        float *vb = v + b * ch * spatial;
        float *ob = attn_out + b * ch * spatial;

        /* Transpose [C, HW] -> [HW, C] */
        for (int c = 0; c < ch; c++) {
            for (int i = 0; i < spatial; i++) {
                q_t[i * ch + c] = qb[c * spatial + i] * scale;
                k_t[i * ch + c] = kb[c * spatial + i];
                v_t[i * ch + c] = vb[c * spatial + i];
            }
        }

        /* Q @ K^T using BLAS: [HW, C] @ [C, HW] -> [HW, HW] */
        flux_matmul_t(scores, q_t, k_t, spatial, ch, spatial);

        /* Softmax */
        flux_softmax(scores, spatial, spatial);

        /* scores @ V using BLAS: [HW, HW] @ [HW, C] -> [HW, C] */
        flux_matmul(o_t, scores, v_t, spatial, spatial, ch);

        /* Transpose output back [HW, C] -> [C, HW] */
        for (int c = 0; c < ch; c++) {
            for (int i = 0; i < spatial; i++) {
                ob[c * spatial + i] = o_t[i * ch + c];
            }
        }
    }

    free(q_t);
    free(k_t);
    free(v_t);
    free(o_t);
    free(scores);

    /* Project output */
    vae_conv2d(work, attn_out, block->out_weight, block->out_bias,
                batch, ch, ch, H, W, 1, 1, 1, 0);

    /* Add residual */
    flux_add(out, x, work, batch * ch * spatial);
    return 0;
}

/* ========================================================================
 * Encoder Forward Pass
 * ======================================================================== */

float *flux_vae_encode(flux_vae_t *vae, const float *img,
                       int batch, int H, int W,
                       int *out_h, int *out_w) {
    /*
     * Encoder path:
     * [B, 3, H, W] -> conv_in -> down_blocks -> mid_block -> norm -> conv_out
     * -> [B, 64, H/8, W/8] (32 mean + 32 logvar, use mean only)
     * -> patchify 2x2 -> [B, 128, H/16, W/16]
     * -> batch_norm
     */

    int ch_mult[4] = {1, 2, 4, 4};
    float *x = vae->work1;
    float *work = vae->work2;

    int cur_h = H, cur_w = W;

    /* Conv in: 3 -> 128 */
    vae_conv2d(x, img, vae->enc_conv_in_weight, vae->enc_conv_in_bias,
                batch, 3, vae->base_channels, H, W, 3, 3, 1, 1);

    int block_idx = 0;
    int down_idx = 0;
    int progress = 0;
    int total_blocks = 4 * vae->num_res_blocks + 3;  /* down resblocks + mid */

    /* Down blocks */
    for (int level = 0; level < 4; level++) {
        int ch_out = vae->base_channels * ch_mult[level];

        for (int r = 0; r < vae->num_res_blocks; r++) {
            vae_resblock_t *block = &vae->enc_down_blocks[block_idx++];
            resblock_forward(work, x, block, vae->work3,
                             batch, cur_h, cur_w, vae->num_groups, vae->eps);
            flux_copy(x, work, batch * ch_out * cur_h * cur_w);
            if (flux_vae_progress_callback)
                flux_vae_progress_callback(progress++, total_blocks);
        }

        /* Downsample (except last level) */
        if (level < 3) {
            vae_downsample_t *ds = &vae->enc_downsample[down_idx++];
            /* Asymmetric padding: pad right and bottom by 1.
             * Implemented explicitly to match training/reference impl. */
            float *padded = vae->work3;
            int padded_h = cur_h + 1;
            int padded_w = cur_w + 1;
            int new_h = (padded_h - 3) / 2 + 1;
            int new_w = (padded_w - 3) / 2 + 1;
            vae_pad_right_bottom(padded, x, batch, ch_out, cur_h, cur_w);
            vae_conv2d(work, padded, ds->conv_weight, ds->conv_bias,
                       batch, ch_out, ch_out, padded_h, padded_w, 3, 3, 2, 0);
            cur_h = new_h;
            cur_w = new_w;
            flux_copy(x, work, batch * ch_out * cur_h * cur_w);
        }
    }

    int mid_ch = vae->base_channels * ch_mult[3];  /* 512 */

    /* Mid block: resblock -> attn -> resblock */
    resblock_forward(work, x, &vae->enc_mid_block1, vae->work3,
                     batch, cur_h, cur_w, vae->num_groups, vae->eps);
    if (flux_vae_progress_callback)
        flux_vae_progress_callback(progress++, total_blocks);
    if (attnblock_forward(x, work, &vae->enc_mid_attn, vae->work3,
                          batch, cur_h, cur_w, vae->num_groups, vae->eps) < 0) {
        return NULL;  /* OOM in attention */
    }
    if (flux_vae_progress_callback)
        flux_vae_progress_callback(progress++, total_blocks);
    resblock_forward(work, x, &vae->enc_mid_block2, vae->work3,
                     batch, cur_h, cur_w, vae->num_groups, vae->eps);
    flux_copy(x, work, batch * mid_ch * cur_h * cur_w);
    if (flux_vae_progress_callback)
        flux_vae_progress_callback(progress++, total_blocks);

    /* Output: norm -> swish -> conv */
    flux_group_norm(work, x, vae->enc_norm_out_weight, vae->enc_norm_out_bias,
                    batch, mid_ch, cur_h, cur_w, vae->num_groups, vae->eps);
    swish_inplace(work, batch * mid_ch * cur_h * cur_w);

    /* Conv out: 512 -> 64 (32 mean + 32 logvar) */
    int z_ch = vae->z_channels * 2;  /* 64 */
    vae_conv2d(x, work, vae->enc_conv_out_weight, vae->enc_conv_out_bias,
                batch, mid_ch, z_ch, cur_h, cur_w, 3, 3, 1, 1);

    /* Quant conv: 64 -> 64 (1x1 conv) */
    vae_conv2d(work, x, vae->quant_conv_weight, vae->quant_conv_bias,
               batch, z_ch, z_ch, cur_h, cur_w, 1, 1, 1, 0);
    flux_copy(x, work, batch * z_ch * cur_h * cur_w);

    /* Take mean only (first 32 channels) */
    /* x is [B, 64, H/8, W/8], we want [B, 32, H/8, W/8] */
    int latent_h = cur_h;
    int latent_w = cur_w;
    int z_spatial = latent_h * latent_w;

    float *mean = (float *)malloc(batch * vae->z_channels * z_spatial * sizeof(float));
    for (int b = 0; b < batch; b++) {
        memcpy(mean + b * vae->z_channels * z_spatial,
               x + b * z_ch * z_spatial,
               vae->z_channels * z_spatial * sizeof(float));
    }

    /* Patchify: [B, 32, H/8, W/8] -> [B, 128, H/16, W/16] */
    int patch_h = latent_h / 2;
    int patch_w = latent_w / 2;
    float *latent = (float *)malloc(batch * FLUX_LATENT_CHANNELS * patch_h * patch_w * sizeof(float));
    flux_patchify(latent, mean, batch, vae->z_channels, latent_h, latent_w, 2);
    free(mean);

    /* Batch normalize */
    flux_batch_norm(work, latent, vae->bn_mean, vae->bn_var, NULL, NULL,
                    batch, FLUX_LATENT_CHANNELS, patch_h, patch_w, vae->eps);
    flux_copy(latent, work, batch * FLUX_LATENT_CHANNELS * patch_h * patch_w);

    *out_h = patch_h;
    *out_w = patch_w;
    return latent;
}

/* ========================================================================
 * GPU-Resident Decoder
 * ======================================================================== */

#ifdef USE_METAL

/* GPU resblock: all operations on GPU, returns new tensor */
static flux_gpu_tensor_t resblock_forward_gpu(flux_gpu_tensor_t x,
                                               const vae_resblock_t *block,
                                               int batch, int H, int W,
                                               int num_groups, float eps) {
    int in_ch = block->in_channels;
    int out_ch = block->out_channels;
    int spatial = H * W;
    int n = batch * out_ch * spatial;

    /* Skip connection */
    flux_gpu_tensor_t skip;
    if (in_ch != out_ch) {
        skip = flux_gpu_conv2d_f32(x, block->skip_weight, block->skip_bias,
                                    batch, in_ch, out_ch, H, W, 1, 1, 1, 0);
    } else {
        skip = flux_gpu_tensor_alloc((size_t)n);
        flux_gpu_copy_f32(skip, x, (size_t)n);
    }
    if (!skip) return NULL;

    /* Main path: norm1 -> swish -> conv1 -> norm2 -> swish -> conv2 */
    flux_gpu_tensor_t work = flux_gpu_tensor_alloc((size_t)batch * in_ch * spatial);
    if (!work) { flux_gpu_tensor_free(skip); return NULL; }

    flux_gpu_group_norm_f32(work, x, block->norm1_weight, block->norm1_bias,
                             batch, in_ch, spatial, num_groups, eps);
    flux_gpu_swish_f32(work, work, batch * in_ch * spatial);

    flux_gpu_tensor_t conv1_out = flux_gpu_conv2d_f32(work, block->conv1_weight, block->conv1_bias,
                                                       batch, in_ch, out_ch, H, W, 3, 3, 1, 1);
    flux_gpu_tensor_free(work);
    if (!conv1_out) { flux_gpu_tensor_free(skip); return NULL; }

    work = flux_gpu_tensor_alloc((size_t)batch * out_ch * spatial);
    if (!work) { flux_gpu_tensor_free(skip); flux_gpu_tensor_free(conv1_out); return NULL; }

    flux_gpu_group_norm_f32(work, conv1_out, block->norm2_weight, block->norm2_bias,
                             batch, out_ch, spatial, num_groups, eps);
    flux_gpu_swish_f32(work, work, batch * out_ch * spatial);
    flux_gpu_tensor_free(conv1_out);

    conv1_out = flux_gpu_conv2d_f32(work, block->conv2_weight, block->conv2_bias,
                                     batch, out_ch, out_ch, H, W, 3, 3, 1, 1);
    flux_gpu_tensor_free(work);
    if (!conv1_out) { flux_gpu_tensor_free(skip); return NULL; }

    /* Residual: skip += conv_out */
    flux_gpu_add_f32(skip, skip, conv1_out, n);
    flux_gpu_tensor_free(conv1_out);

    return skip;
}

/* GPU-resident VAE decode.
 * Keeps all data on GPU, only syncs for mid-block attention (CPU) and final output.
 * Returns NULL on failure (caller falls back to CPU path). */
static flux_image *vae_decode_gpu(flux_vae_t *vae, const float *latent,
                                   int batch, int latent_h, int latent_w) {
    if (!flux_metal_available()) return NULL;

    int ch_mult[4] = {1, 2, 4, 4};

    /* Ensure work buffers are allocated (needed for CPU portions of GPU path) */
    int out_H = latent_h * 16;
    int out_W = latent_w * 16;
    if (vae_ensure_work_buffers(vae, out_H, out_W) < 0) return NULL;

    /* Batch denormalize + unpatchify on CPU (small data, fast) */
    float *cpu_x = vae->work1;
    float *cpu_work = vae->work2;

    int z_spatial = latent_h * latent_w;
    flux_copy(cpu_x, latent, batch * FLUX_LATENT_CHANNELS * z_spatial);
    for (int b = 0; b < batch; b++) {
        for (int c = 0; c < FLUX_LATENT_CHANNELS; c++) {
            float mean = vae->bn_mean[c];
            float std = sqrtf(vae->bn_var[c] + vae->eps);
            for (int i = 0; i < z_spatial; i++) {
                int idx = b * FLUX_LATENT_CHANNELS * z_spatial + c * z_spatial + i;
                cpu_x[idx] = cpu_x[idx] * std + mean;
            }
        }
    }

    int unpatch_h = latent_h * 2;
    int unpatch_w = latent_w * 2;
    flux_unpatchify(cpu_work, cpu_x, batch, vae->z_channels, latent_h, latent_w, 2);
    flux_copy(cpu_x, cpu_work, batch * vae->z_channels * unpatch_h * unpatch_w);
    int cur_h = unpatch_h, cur_w = unpatch_w;

    /* Upload to GPU and start batch */
    size_t x_size = (size_t)batch * vae->z_channels * cur_h * cur_w;
    flux_gpu_tensor_t x = flux_gpu_tensor_create(cpu_x, x_size);
    if (!x) return NULL;

    flux_gpu_batch_begin();

    /* Post-quantization conv (1x1): 32 -> 32 */
    flux_gpu_tensor_t t = flux_gpu_conv2d_f32(x, vae->post_quant_conv_weight, vae->post_quant_conv_bias,
                                               batch, vae->z_channels, vae->z_channels, cur_h, cur_w, 1, 1, 1, 0);
    flux_gpu_tensor_free(x);
    if (!t) { flux_gpu_batch_end(); return NULL; }
    x = t;

    /* Conv in: 32 -> 512 */
    int mid_ch = vae->base_channels * ch_mult[3];
    t = flux_gpu_conv2d_f32(x, vae->dec_conv_in_weight, vae->dec_conv_in_bias,
                             batch, vae->z_channels, mid_ch, cur_h, cur_w, 3, 3, 1, 1);
    flux_gpu_tensor_free(x);
    if (!t) { flux_gpu_batch_end(); return NULL; }
    x = t;

    /* Mid block: resblock1 */
    int progress = 0;
    int total_blocks = 3 + 4 * (vae->num_res_blocks + 1);

    t = resblock_forward_gpu(x, &vae->dec_mid_block1, batch, cur_h, cur_w, vae->num_groups, vae->eps);
    flux_gpu_tensor_free(x);
    if (!t) { flux_gpu_batch_end(); return NULL; }
    x = t;
    if (flux_vae_progress_callback) flux_vae_progress_callback(progress++, total_blocks);

    /* Mid block attention: sync to CPU, run attention, upload back */
    {
        size_t attn_size = (size_t)batch * mid_ch * cur_h * cur_w;
        float *cpu_attn_in = cpu_work;

        flux_gpu_batch_end();  /* Sync: execute everything queued so far */

        /* Download GPU tensor to CPU */
        flux_gpu_tensor_read(x, cpu_attn_in);

        /* Run attention on CPU (uses existing attnblock_forward) */
        float *cpu_attn_out = cpu_x;
        if (attnblock_forward(cpu_attn_out, cpu_attn_in, &vae->dec_mid_attn,
                               vae->work3, batch, cur_h, cur_w,
                               vae->num_groups, vae->eps) < 0) {
            flux_gpu_tensor_free(x);
            return NULL;
        }
        if (flux_vae_progress_callback) flux_vae_progress_callback(progress++, total_blocks);

        /* Upload result back to GPU */
        flux_gpu_tensor_free(x);
        x = flux_gpu_tensor_create(cpu_attn_out, attn_size);
        if (!x) return NULL;

        flux_gpu_batch_begin();  /* Start new batch for remaining work */
    }

    /* Mid block: resblock2 */
    t = resblock_forward_gpu(x, &vae->dec_mid_block2, batch, cur_h, cur_w, vae->num_groups, vae->eps);
    flux_gpu_tensor_free(x);
    if (!t) { flux_gpu_batch_end(); return NULL; }
    x = t;
    if (flux_vae_progress_callback) flux_vae_progress_callback(progress++, total_blocks);

    int block_idx = 0;
    int up_idx = 0;

    /* Up blocks (reverse order of channels) */
    for (int level = 3; level >= 0; level--) {
        int ch_out = vae->base_channels * ch_mult[level];

        for (int r = 0; r < vae->num_res_blocks + 1; r++) {
            vae_resblock_t *block = &vae->dec_up_blocks[block_idx++];
            t = resblock_forward_gpu(x, block, batch, cur_h, cur_w, vae->num_groups, vae->eps);
            flux_gpu_tensor_free(x);
            if (!t) { flux_gpu_batch_end(); return NULL; }
            x = t;
            if (flux_vae_progress_callback) flux_vae_progress_callback(progress++, total_blocks);
        }

        /* Upsample (except level 0) */
        if (level > 0) {
            vae_upsample_t *us = &vae->dec_upsample[up_idx++];
            int new_h = cur_h * 2;
            int new_w = cur_w * 2;

            t = flux_gpu_upsample_nearest_2x_f32(x, ch_out, cur_h, cur_w);
            flux_gpu_tensor_free(x);
            if (!t) { flux_gpu_batch_end(); return NULL; }

            x = flux_gpu_conv2d_f32(t, us->conv_weight, us->conv_bias,
                                     batch, ch_out, ch_out, new_h, new_w, 3, 3, 1, 1);
            flux_gpu_tensor_free(t);
            if (!x) { flux_gpu_batch_end(); return NULL; }

            cur_h = new_h;
            cur_w = new_w;
        }
    }

    /* Output: norm -> swish -> conv_out */
    int out_ch = vae->base_channels;  /* 128 */
    size_t final_size = (size_t)batch * out_ch * cur_h * cur_w;
    t = flux_gpu_tensor_alloc(final_size);
    if (!t) { flux_gpu_tensor_free(x); flux_gpu_batch_end(); return NULL; }
    flux_gpu_group_norm_f32(t, x, vae->dec_norm_out_weight, vae->dec_norm_out_bias,
                             batch, out_ch, cur_h * cur_w, vae->num_groups, vae->eps);
    flux_gpu_swish_f32(t, t, (int)final_size);
    flux_gpu_tensor_free(x);

    x = flux_gpu_conv2d_f32(t, vae->dec_conv_out_weight, vae->dec_conv_out_bias,
                             batch, out_ch, 3, cur_h, cur_w, 3, 3, 1, 1);
    flux_gpu_tensor_free(t);
    if (!x) { flux_gpu_batch_end(); return NULL; }

    /* Execute everything and read result */
    flux_gpu_batch_end();

    int H = cur_h;
    int W = cur_w;
    size_t rgb_size = (size_t)batch * 3 * H * W;
    float *rgb = (float *)malloc(rgb_size * sizeof(float));
    if (!rgb) { flux_gpu_tensor_free(x); return NULL; }
    flux_gpu_tensor_read(x, rgb);
    flux_gpu_tensor_free(x);

    /* Convert to image */
    flux_image *img = flux_image_create(W, H, 3);
    if (!img) { free(rgb); return NULL; }

    for (int y = 0; y < H; y++) {
        for (int c = 0; c < W; c++) {
            for (int ch = 0; ch < 3; ch++) {
                float val = rgb[ch * H * W + y * W + c];
                val = (val + 1.0f) * 0.5f;
                val = val * 255.0f;
                if (val < 0) val = 0;
                if (val > 255) val = 255;
                img->data[(y * W + c) * 3 + ch] = (unsigned char)(val + 0.5f);
            }
        }
    }

    free(rgb);
    return img;
}

#endif /* USE_METAL */

/* ========================================================================
 * Decoder Forward Pass
 * ======================================================================== */

flux_image *flux_vae_decode(flux_vae_t *vae, const float *latent,
                            int batch, int latent_h, int latent_w) {
#ifdef USE_METAL
    /* Try GPU-resident path first (eliminates CPU<->GPU round-trips per conv) */
    if (flux_metal_available()) {
        flux_image *gpu_result = vae_decode_gpu(vae, latent, batch, latent_h, latent_w);
        if (gpu_result) return gpu_result;
        /* Fall through to CPU path on failure */
    }
#endif

    /*
     * Decoder path:
     * [B, 128, H/16, W/16]
     * -> batch_denorm
     * -> unpatchify -> [B, 32, H/8, W/8]
     * -> conv_in -> mid_block -> up_blocks -> norm -> conv_out
     * -> [B, 3, H, W]
     */

    int ch_mult[4] = {1, 2, 4, 4};
    float *x = vae->work1;
    float *work = vae->work2;

    /* Batch denormalize */
    int z_spatial = latent_h * latent_w;
    flux_copy(x, latent, batch * FLUX_LATENT_CHANNELS * z_spatial);

    /* Denormalize: x = x * sqrt(var + eps) + mean */
    for (int b = 0; b < batch; b++) {
        for (int c = 0; c < FLUX_LATENT_CHANNELS; c++) {
            float mean = vae->bn_mean[c];
            float std = sqrtf(vae->bn_var[c] + vae->eps);
            for (int i = 0; i < z_spatial; i++) {
                int idx = b * FLUX_LATENT_CHANNELS * z_spatial + c * z_spatial + i;
                x[idx] = x[idx] * std + mean;
            }
        }
    }

    /* Unpatchify: [B, 128, H/16, W/16] -> [B, 32, H/8, W/8] */
    int unpatch_h = latent_h * 2;
    int unpatch_w = latent_w * 2;
    flux_unpatchify(work, x, batch, vae->z_channels, latent_h, latent_w, 2);
    flux_copy(x, work, batch * vae->z_channels * unpatch_h * unpatch_w);

    int cur_h = unpatch_h, cur_w = unpatch_w;

    /* Post-quantization conv (1x1): 32 -> 32 */
    vae_conv2d(work, x, vae->post_quant_conv_weight, vae->post_quant_conv_bias,
                batch, vae->z_channels, vae->z_channels, cur_h, cur_w, 1, 1, 1, 0);
    flux_copy(x, work, batch * vae->z_channels * cur_h * cur_w);

    /* Conv in: 32 -> 512 */
    int mid_ch = vae->base_channels * ch_mult[3];  /* 512 */
    vae_conv2d(work, x, vae->dec_conv_in_weight, vae->dec_conv_in_bias,
                batch, vae->z_channels, mid_ch, cur_h, cur_w, 3, 3, 1, 1);
    flux_copy(x, work, batch * mid_ch * cur_h * cur_w);

    /* Mid block: resblock -> attn -> resblock */
    int progress = 0;
    int total_blocks = 3 + 4 * (vae->num_res_blocks + 1);  /* mid + up resblocks */

    resblock_forward(work, x, &vae->dec_mid_block1, vae->work3,
                     batch, cur_h, cur_w, vae->num_groups, vae->eps);
    if (flux_vae_progress_callback)
        flux_vae_progress_callback(progress++, total_blocks);
    if (attnblock_forward(x, work, &vae->dec_mid_attn, vae->work3,
                          batch, cur_h, cur_w, vae->num_groups, vae->eps) < 0) {
        return NULL;  /* OOM in attention */
    }
    if (flux_vae_progress_callback)
        flux_vae_progress_callback(progress++, total_blocks);
    resblock_forward(work, x, &vae->dec_mid_block2, vae->work3,
                     batch, cur_h, cur_w, vae->num_groups, vae->eps);
    flux_copy(x, work, batch * mid_ch * cur_h * cur_w);
    if (flux_vae_progress_callback)
        flux_vae_progress_callback(progress++, total_blocks);

    int block_idx = 0;
    int up_idx = 0;

    /* Up blocks (reverse order of channels) */
    for (int level = 3; level >= 0; level--) {
        int ch_out = vae->base_channels * ch_mult[level];

        /* num_res_blocks + 1 resblocks per level */
        for (int r = 0; r < vae->num_res_blocks + 1; r++) {
            vae_resblock_t *block = &vae->dec_up_blocks[block_idx++];
            resblock_forward(work, x, block, vae->work3,
                             batch, cur_h, cur_w, vae->num_groups, vae->eps);
            flux_copy(x, work, batch * ch_out * cur_h * cur_w);
            if (flux_vae_progress_callback)
                flux_vae_progress_callback(progress++, total_blocks);
        }

        /* Upsample (except level 0) */
        if (level > 0) {
            vae_upsample_t *us = &vae->dec_upsample[up_idx++];
            int new_h = cur_h * 2;
            int new_w = cur_w * 2;

            /* Nearest neighbor upsample */
            flux_upsample_nearest(work, x, batch, ch_out, cur_h, cur_w, 2, 2);

            /* Conv for refinement */
            vae_conv2d(x, work, us->conv_weight, us->conv_bias,
                        batch, ch_out, ch_out, new_h, new_w, 3, 3, 1, 1);

            cur_h = new_h;
            cur_w = new_w;
        }
    }

    int out_ch = vae->base_channels;  /* 128 */

    /* Output: norm -> swish -> conv */
    flux_group_norm(work, x, vae->dec_norm_out_weight, vae->dec_norm_out_bias,
                    batch, out_ch, cur_h, cur_w, vae->num_groups, vae->eps);
    swish_inplace(work, batch * out_ch * cur_h * cur_w);

    /* Conv out: 128 -> 3 */
    vae_conv2d(x, work, vae->dec_conv_out_weight, vae->dec_conv_out_bias,
                batch, out_ch, 3, cur_h, cur_w, 3, 3, 1, 1);

    /* Convert to image */
    int H = cur_h;
    int W = cur_w;

    flux_image *img = flux_image_create(W, H, 3);
    if (!img) return NULL;

    /* Denormalize from [-1, 1] to [0, 255] and convert to uint8 */
    for (int y = 0; y < H; y++) {
        for (int c = 0; c < W; c++) {
            for (int ch = 0; ch < 3; ch++) {
                float val = x[ch * H * W + y * W + c];
                val = (val + 1.0f) * 0.5f;  /* [-1,1] -> [0,1] */
                val = val * 255.0f;
                if (val < 0) val = 0;
                if (val > 255) val = 255;
                img->data[(y * W + c) * 3 + ch] = (uint8_t)val;
            }
        }
    }

    return img;
}

/* ========================================================================
 * VAE Loading and Memory Management
 * ======================================================================== */

static int read_uint32(FILE *f, uint32_t *val) {
    return fread(val, sizeof(uint32_t), 1, f) == 1;
}

static float *read_floats(FILE *f, int count) {
    float *data = (float *)malloc(count * sizeof(float));
    if (!data) return NULL;
    if (fread(data, sizeof(float), count, f) != (size_t)count) {
        free(data);
        return NULL;
    }
    return data;
}

static int load_resblock(FILE *f, vae_resblock_t *block) {
    if (!read_uint32(f, (uint32_t *)&block->in_channels)) return 0;
    if (!read_uint32(f, (uint32_t *)&block->out_channels)) return 0;

    int in_ch = block->in_channels;
    int out_ch = block->out_channels;

    block->norm1_weight = read_floats(f, in_ch);
    block->norm1_bias = read_floats(f, in_ch);
    block->conv1_weight = read_floats(f, out_ch * in_ch * 3 * 3);
    block->conv1_bias = read_floats(f, out_ch);
    block->norm2_weight = read_floats(f, out_ch);
    block->norm2_bias = read_floats(f, out_ch);
    block->conv2_weight = read_floats(f, out_ch * out_ch * 3 * 3);
    block->conv2_bias = read_floats(f, out_ch);

    if (in_ch != out_ch) {
        block->skip_weight = read_floats(f, out_ch * in_ch);
        block->skip_bias = read_floats(f, out_ch);
    } else {
        block->skip_weight = NULL;
        block->skip_bias = NULL;
    }

    return block->norm1_weight && block->conv1_weight && block->conv2_weight;
}

static int load_attnblock(FILE *f, vae_attnblock_t *block) {
    if (!read_uint32(f, (uint32_t *)&block->channels)) return 0;

    int ch = block->channels;

    block->norm_weight = read_floats(f, ch);
    block->norm_bias = read_floats(f, ch);
    block->q_weight = read_floats(f, ch * ch);
    block->q_bias = read_floats(f, ch);
    block->k_weight = read_floats(f, ch * ch);
    block->k_bias = read_floats(f, ch);
    block->v_weight = read_floats(f, ch * ch);
    block->v_bias = read_floats(f, ch);
    block->out_weight = read_floats(f, ch * ch);
    block->out_bias = read_floats(f, ch);

    return block->norm_weight && block->q_weight && block->out_weight;
}

static void free_resblock(vae_resblock_t *block) {
    free(block->norm1_weight);
    free(block->norm1_bias);
    free(block->conv1_weight);
    free(block->conv1_bias);
    free(block->norm2_weight);
    free(block->norm2_bias);
    free(block->conv2_weight);
    free(block->conv2_bias);
    free(block->skip_weight);
    free(block->skip_bias);
}

static void free_attnblock(vae_attnblock_t *block) {
    free(block->norm_weight);
    free(block->norm_bias);
    free(block->q_weight);
    free(block->q_bias);
    free(block->k_weight);
    free(block->k_bias);
    free(block->v_weight);
    free(block->v_bias);
    free(block->out_weight);
    free(block->out_bias);
}

flux_vae_t *flux_vae_load(FILE *f) {
    flux_vae_t *vae = calloc(1, sizeof(flux_vae_t));
    if (!vae) return NULL;

    /* Read config */
    uint32_t config[6];
    if (fread(config, sizeof(uint32_t), 6, f) != 6) goto error;

    vae->z_channels = config[0];
    vae->base_channels = config[1];
    vae->num_res_blocks = config[2];
    vae->num_groups = config[3];
    vae->max_h = config[4];
    vae->max_w = config[5];

    vae->ch_mult[0] = FLUX_VAE_CH_MULT_0;
    vae->ch_mult[1] = FLUX_VAE_CH_MULT_1;
    vae->ch_mult[2] = FLUX_VAE_CH_MULT_2;
    vae->ch_mult[3] = FLUX_VAE_CH_MULT_3;
    vae->eps = 1e-4f;  /* batch_norm_eps from config */

    /* Read encoder conv_in */
    vae->enc_conv_in_weight = read_floats(f, vae->base_channels * 3 * 3 * 3);
    vae->enc_conv_in_bias = read_floats(f, vae->base_channels);

    /* Read encoder down blocks */
    int num_down_blocks = 4 * vae->num_res_blocks;
    vae->enc_down_blocks = calloc(num_down_blocks, sizeof(vae_resblock_t));
    for (int i = 0; i < num_down_blocks; i++) {
        if (!load_resblock(f, &vae->enc_down_blocks[i])) goto error;
    }

    /* Read encoder downsamples */
    vae->enc_downsample = calloc(3, sizeof(vae_downsample_t));
    for (int i = 0; i < 3; i++) {
        int ch = vae->base_channels * vae->ch_mult[i];
        vae->enc_downsample[i].channels = ch;
        vae->enc_downsample[i].conv_weight = read_floats(f, ch * ch * 3 * 3);
        vae->enc_downsample[i].conv_bias = read_floats(f, ch);
    }

    /* Read encoder mid block */
    if (!load_resblock(f, &vae->enc_mid_block1)) goto error;
    if (!load_attnblock(f, &vae->enc_mid_attn)) goto error;
    if (!load_resblock(f, &vae->enc_mid_block2)) goto error;

    /* Read encoder output */
    int mid_ch = vae->base_channels * vae->ch_mult[3];
    vae->enc_norm_out_weight = read_floats(f, mid_ch);
    vae->enc_norm_out_bias = read_floats(f, mid_ch);
    vae->enc_conv_out_weight = read_floats(f, vae->z_channels * 2 * mid_ch * 3 * 3);
    vae->enc_conv_out_bias = read_floats(f, vae->z_channels * 2);

    /* Read decoder conv_in */
    vae->dec_conv_in_weight = read_floats(f, mid_ch * vae->z_channels * 3 * 3);
    vae->dec_conv_in_bias = read_floats(f, mid_ch);

    /* Read decoder mid block */
    if (!load_resblock(f, &vae->dec_mid_block1)) goto error;
    if (!load_attnblock(f, &vae->dec_mid_attn)) goto error;
    if (!load_resblock(f, &vae->dec_mid_block2)) goto error;

    /* Read decoder up blocks */
    int num_up_blocks = 4 * (vae->num_res_blocks + 1);
    vae->dec_up_blocks = calloc(num_up_blocks, sizeof(vae_resblock_t));
    for (int i = 0; i < num_up_blocks; i++) {
        if (!load_resblock(f, &vae->dec_up_blocks[i])) goto error;
    }

    /* Read decoder upsamples */
    vae->dec_upsample = calloc(3, sizeof(vae_upsample_t));
    for (int i = 0; i < 3; i++) {
        int ch = vae->base_channels * vae->ch_mult[3 - i];
        vae->dec_upsample[i].channels = ch;
        vae->dec_upsample[i].conv_weight = read_floats(f, ch * ch * 3 * 3);
        vae->dec_upsample[i].conv_bias = read_floats(f, ch);
    }

    /* Read decoder output */
    vae->dec_norm_out_weight = read_floats(f, vae->base_channels);
    vae->dec_norm_out_bias = read_floats(f, vae->base_channels);
    vae->dec_conv_out_weight = read_floats(f, 3 * vae->base_channels * 3 * 3);
    vae->dec_conv_out_bias = read_floats(f, 3);

    /* Read batch norm stats */
    vae->bn_mean = read_floats(f, FLUX_LATENT_CHANNELS);
    vae->bn_var = read_floats(f, FLUX_LATENT_CHANNELS);

    /* Allocate working memory */
    size_t max_spatial = (size_t)vae->max_h * vae->max_w;
    size_t max_channels = mid_ch;  /* 512 */
    vae->work_size = 4 * max_channels * max_spatial * sizeof(float);
    vae->work1 = (float *)malloc(vae->work_size);
    vae->work2 = (float *)malloc(vae->work_size);
    vae->work3 = (float *)malloc(vae->work_size);

    if (!vae->work1 || !vae->work2 || !vae->work3) goto error;

    return vae;

error:
    flux_vae_free(vae);
    return NULL;
}

void flux_vae_free(flux_vae_t *vae) {
    if (!vae) return;

    free(vae->enc_conv_in_weight);
    free(vae->enc_conv_in_bias);

    if (vae->enc_down_blocks) {
        for (int i = 0; i < 4 * vae->num_res_blocks; i++) {
            free_resblock(&vae->enc_down_blocks[i]);
        }
        free(vae->enc_down_blocks);
    }

    if (vae->enc_downsample) {
        for (int i = 0; i < 3; i++) {
            free(vae->enc_downsample[i].conv_weight);
            free(vae->enc_downsample[i].conv_bias);
        }
        free(vae->enc_downsample);
    }

    free_resblock(&vae->enc_mid_block1);
    free_attnblock(&vae->enc_mid_attn);
    free_resblock(&vae->enc_mid_block2);

    free(vae->enc_norm_out_weight);
    free(vae->enc_norm_out_bias);
    free(vae->enc_conv_out_weight);
    free(vae->enc_conv_out_bias);
    free(vae->quant_conv_weight);
    free(vae->quant_conv_bias);

    free(vae->dec_conv_in_weight);
    free(vae->dec_conv_in_bias);

    free_resblock(&vae->dec_mid_block1);
    free_attnblock(&vae->dec_mid_attn);
    free_resblock(&vae->dec_mid_block2);

    if (vae->dec_up_blocks) {
        for (int i = 0; i < 4 * (vae->num_res_blocks + 1); i++) {
            free_resblock(&vae->dec_up_blocks[i]);
        }
        free(vae->dec_up_blocks);
    }

    if (vae->dec_upsample) {
        for (int i = 0; i < 3; i++) {
            free(vae->dec_upsample[i].conv_weight);
            free(vae->dec_upsample[i].conv_bias);
        }
        free(vae->dec_upsample);
    }

    free(vae->dec_norm_out_weight);
    free(vae->dec_norm_out_bias);
    free(vae->dec_conv_out_weight);
    free(vae->dec_conv_out_bias);

    free(vae->bn_mean);
    free(vae->bn_var);
    free(vae->post_quant_conv_weight);
    free(vae->post_quant_conv_bias);

    free(vae->work1);
    free(vae->work2);
    free(vae->work3);

    free(vae);
}

/* ========================================================================
 * Image Preprocessing
 * ======================================================================== */

/* Convert image to tensor [B, 3, H, W] normalized to [-1, 1] */
float *flux_image_to_tensor(const flux_image *img) {
    int H = img->height;
    int W = img->width;
    int C = img->channels;

    float *tensor = (float *)malloc(3 * H * W * sizeof(float));
    if (!tensor) return NULL;

    for (int y = 0; y < H; y++) {
        for (int x = 0; x < W; x++) {
            for (int c = 0; c < 3; c++) {
                float val;
                if (c < C) {
                    val = (float)img->data[(y * W + x) * C + c];
                } else {
                    val = 0.0f;  /* Pad with zeros if grayscale */
                }
                val = val / 255.0f;         /* [0, 255] -> [0, 1] */
                val = val * 2.0f - 1.0f;    /* [0, 1] -> [-1, 1] */
                tensor[c * H * W + y * W + x] = val;
            }
        }
    }

    return tensor;
}

/* ========================================================================
 * Safetensors Loading
 * ======================================================================== */

static float *get_sf_tensor(safetensors_file_t *sf, const char *name) {
    const safetensor_t *t = safetensors_find(sf, name);
    if (!t) {
        fprintf(stderr, "Error: required tensor %s not found\n", name);
        return NULL;
    }
    return safetensors_get_f32(sf, t);
}

static int load_resblock_sf(safetensors_file_t *sf, vae_resblock_t *block,
                             const char *prefix, int in_ch, int out_ch) {
    char name[256];

    block->in_channels = in_ch;
    block->out_channels = out_ch;

    snprintf(name, sizeof(name), "%s.norm1.weight", prefix);
    block->norm1_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.norm1.bias", prefix);
    block->norm1_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.conv1.weight", prefix);
    block->conv1_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.conv1.bias", prefix);
    block->conv1_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.norm2.weight", prefix);
    block->norm2_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.norm2.bias", prefix);
    block->norm2_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.conv2.weight", prefix);
    block->conv2_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.conv2.bias", prefix);
    block->conv2_bias = get_sf_tensor(sf, name);

    if (in_ch != out_ch) {
        snprintf(name, sizeof(name), "%s.conv_shortcut.weight", prefix);
        block->skip_weight = get_sf_tensor(sf, name);
        snprintf(name, sizeof(name), "%s.conv_shortcut.bias", prefix);
        block->skip_bias = get_sf_tensor(sf, name);
    } else {
        block->skip_weight = NULL;
        block->skip_bias = NULL;
    }

    /* Check required tensors */
    if (!block->norm1_weight || !block->conv1_weight || !block->conv2_weight) {
        return -1;
    }
    return 0;
}

static int load_attnblock_sf(safetensors_file_t *sf, vae_attnblock_t *block,
                              const char *prefix, int channels) {
    char name[256];

    block->channels = channels;

    snprintf(name, sizeof(name), "%s.group_norm.weight", prefix);
    block->norm_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.group_norm.bias", prefix);
    block->norm_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.to_q.weight", prefix);
    block->q_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.to_q.bias", prefix);
    block->q_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.to_k.weight", prefix);
    block->k_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.to_k.bias", prefix);
    block->k_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.to_v.weight", prefix);
    block->v_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.to_v.bias", prefix);
    block->v_bias = get_sf_tensor(sf, name);

    snprintf(name, sizeof(name), "%s.to_out.0.weight", prefix);
    block->out_weight = get_sf_tensor(sf, name);
    snprintf(name, sizeof(name), "%s.to_out.0.bias", prefix);
    block->out_bias = get_sf_tensor(sf, name);

    /* Check required tensors */
    if (!block->norm_weight || !block->q_weight || !block->out_weight) {
        return -1;
    }
    return 0;
}

flux_vae_t *flux_vae_load_safetensors(safetensors_file_t *sf) {
    flux_vae_t *vae = calloc(1, sizeof(flux_vae_t));
    if (!vae) return NULL;

    char name[256];
    int ch_mult[4] = {1, 2, 4, 4};

    /* Set config */
    vae->z_channels = 32;
    vae->base_channels = 128;
    vae->num_res_blocks = 2;
    vae->num_groups = 32;
    vae->max_h = FLUX_VAE_MAX_DIM;
    vae->max_w = FLUX_VAE_MAX_DIM;
    vae->eps = 1e-4f;  /* batch_norm_eps from config */

    vae->ch_mult[0] = ch_mult[0];
    vae->ch_mult[1] = ch_mult[1];
    vae->ch_mult[2] = ch_mult[2];
    vae->ch_mult[3] = ch_mult[3];

    /* Encoder conv_in */
    vae->enc_conv_in_weight = get_sf_tensor(sf, "encoder.conv_in.weight");
    vae->enc_conv_in_bias = get_sf_tensor(sf, "encoder.conv_in.bias");

    /* Encoder down blocks */
    int num_down_blocks = 4 * vae->num_res_blocks;
    vae->enc_down_blocks = calloc(num_down_blocks, sizeof(vae_resblock_t));

    int block_idx = 0;
    for (int level = 0; level < 4; level++) {
        int ch = vae->base_channels * ch_mult[level];
        int prev_ch = (level == 0) ? vae->base_channels : vae->base_channels * ch_mult[level - 1];

        for (int r = 0; r < vae->num_res_blocks; r++) {
            int in_ch = (r == 0 && level > 0) ? prev_ch : ch;
            snprintf(name, sizeof(name), "encoder.down_blocks.%d.resnets.%d", level, r);
            load_resblock_sf(sf, &vae->enc_down_blocks[block_idx++], name, in_ch, ch);
        }
    }

    /* Encoder downsamples */
    vae->enc_downsample = calloc(3, sizeof(vae_downsample_t));
    for (int i = 0; i < 3; i++) {
        int ch = vae->base_channels * ch_mult[i];
        vae->enc_downsample[i].channels = ch;
        snprintf(name, sizeof(name), "encoder.down_blocks.%d.downsamplers.0.conv.weight", i);
        vae->enc_downsample[i].conv_weight = get_sf_tensor(sf, name);
        snprintf(name, sizeof(name), "encoder.down_blocks.%d.downsamplers.0.conv.bias", i);
        vae->enc_downsample[i].conv_bias = get_sf_tensor(sf, name);
    }

    /* Encoder mid block */
    int mid_ch = vae->base_channels * ch_mult[3];  /* 512 */
    load_resblock_sf(sf, &vae->enc_mid_block1, "encoder.mid_block.resnets.0", mid_ch, mid_ch);
    load_attnblock_sf(sf, &vae->enc_mid_attn, "encoder.mid_block.attentions.0", mid_ch);
    load_resblock_sf(sf, &vae->enc_mid_block2, "encoder.mid_block.resnets.1", mid_ch, mid_ch);

    /* Encoder output */
    vae->enc_norm_out_weight = get_sf_tensor(sf, "encoder.conv_norm_out.weight");
    vae->enc_norm_out_bias = get_sf_tensor(sf, "encoder.conv_norm_out.bias");
    vae->enc_conv_out_weight = get_sf_tensor(sf, "encoder.conv_out.weight");
    vae->enc_conv_out_bias = get_sf_tensor(sf, "encoder.conv_out.bias");
    vae->quant_conv_weight = get_sf_tensor(sf, "quant_conv.weight");
    vae->quant_conv_bias = get_sf_tensor(sf, "quant_conv.bias");

    /* Decoder conv_in */
    vae->dec_conv_in_weight = get_sf_tensor(sf, "decoder.conv_in.weight");
    vae->dec_conv_in_bias = get_sf_tensor(sf, "decoder.conv_in.bias");

    /* Decoder mid block */
    load_resblock_sf(sf, &vae->dec_mid_block1, "decoder.mid_block.resnets.0", mid_ch, mid_ch);
    load_attnblock_sf(sf, &vae->dec_mid_attn, "decoder.mid_block.attentions.0", mid_ch);
    load_resblock_sf(sf, &vae->dec_mid_block2, "decoder.mid_block.resnets.1", mid_ch, mid_ch);

    /* Decoder up blocks (reverse order) */
    int num_up_blocks = 4 * (vae->num_res_blocks + 1);
    vae->dec_up_blocks = calloc(num_up_blocks, sizeof(vae_resblock_t));

    block_idx = 0;
    for (int level = 3; level >= 0; level--) {
        int ch = vae->base_channels * ch_mult[level];
        int prev_ch = (level == 3) ? mid_ch : vae->base_channels * ch_mult[level + 1];

        for (int r = 0; r < vae->num_res_blocks + 1; r++) {
            int in_ch = (r == 0) ? prev_ch : ch;
            int up_idx = 3 - level;
            snprintf(name, sizeof(name), "decoder.up_blocks.%d.resnets.%d", up_idx, r);
            load_resblock_sf(sf, &vae->dec_up_blocks[block_idx++], name, in_ch, ch);
        }
    }

    /* Decoder upsamples - up_blocks[0,1,2] have upsamplers, up_blocks[3] does not
     * dec_upsample[0] -> up_blocks.0 (level 3, 512 ch)
     * dec_upsample[1] -> up_blocks.1 (level 2, 512 ch)
     * dec_upsample[2] -> up_blocks.2 (level 1, 256 ch) */
    vae->dec_upsample = calloc(3, sizeof(vae_upsample_t));
    for (int i = 0; i < 3; i++) {
        int ch = vae->base_channels * ch_mult[3 - i];
        vae->dec_upsample[i].channels = ch;
        snprintf(name, sizeof(name), "decoder.up_blocks.%d.upsamplers.0.conv.weight", i);
        vae->dec_upsample[i].conv_weight = get_sf_tensor(sf, name);
        snprintf(name, sizeof(name), "decoder.up_blocks.%d.upsamplers.0.conv.bias", i);
        vae->dec_upsample[i].conv_bias = get_sf_tensor(sf, name);
    }

    /* Decoder output */
    vae->dec_norm_out_weight = get_sf_tensor(sf, "decoder.conv_norm_out.weight");
    vae->dec_norm_out_bias = get_sf_tensor(sf, "decoder.conv_norm_out.bias");
    vae->dec_conv_out_weight = get_sf_tensor(sf, "decoder.conv_out.weight");
    vae->dec_conv_out_bias = get_sf_tensor(sf, "decoder.conv_out.bias");

    /* Batch norm stats */
    const safetensor_t *bn_mean_t = safetensors_find(sf, "bn.running_mean");
    if (bn_mean_t) {
        vae->bn_mean = safetensors_get_f32(sf, bn_mean_t);
        const safetensor_t *bn_var_t = safetensors_find(sf, "bn.running_var");
        vae->bn_var = bn_var_t ? safetensors_get_f32(sf, bn_var_t) : NULL;
    }

    /* Post-quantization conv (32 -> 32, 1x1) */
    vae->post_quant_conv_weight = get_sf_tensor(sf, "post_quant_conv.weight");
    vae->post_quant_conv_bias = get_sf_tensor(sf, "post_quant_conv.bias");
    if (!vae->bn_mean) {
        vae->bn_mean = calloc(FLUX_LATENT_CHANNELS, sizeof(float));
    }
    if (!vae->bn_var) {
        vae->bn_var = malloc(FLUX_LATENT_CHANNELS * sizeof(float));
        for (int i = 0; i < FLUX_LATENT_CHANNELS; i++) vae->bn_var[i] = 1.0f;
    }

    /* Allocate working memory
     * The decoder upsamples from H/8 to full H resolution.
     * At full resolution (level 0), we have base_channels (128) channels.
     * work1/work2: hold main tensors, max 128 * H * W
     * work3: used for resblock/attention ops, needs ~4x main buffer
     *
     * Memory per buffer = 4 * 128 * H * W = 512 * H * W floats
     * For 1024x1024: ~2GB per buffer, ~6GB total working memory
     * For 1792x1792: ~6GB per buffer, ~18GB total working memory
     */
    size_t max_spatial = (size_t)vae->max_h * vae->max_w;
    size_t max_channels = vae->base_channels;  /* 128 at full resolution */
    vae->work_size = 4 * max_channels * max_spatial * sizeof(float);
    vae->work1 = malloc(vae->work_size);
    vae->work2 = malloc(vae->work_size);
    vae->work3 = malloc(vae->work_size);

    if (!vae->work1 || !vae->work2 || !vae->work3) {
        flux_vae_free(vae);
        return NULL;
    }

    return vae;
}
